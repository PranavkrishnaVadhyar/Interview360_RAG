{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PranavkrishnaVadhyar/Interview360_RAG/blob/main/RAG_Using_OpenAI_Embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pn3YLko4ADzr"
      },
      "outputs": [],
      "source": [
        "!pip install unstructured\n",
        "!pip install \"unstructured[pdf]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0BNSWWIBiLz",
        "outputId": "41fdc0bf-99f5-4579-f6bf-d9ffdacc741d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 696 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.3 [186 kB]\n",
            "Fetched 186 kB in 0s (618 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 121752 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.3_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.3) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.3) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "W: Operation was interrupted before it could finish\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get install poppler-utils\n",
        "!sudo apt -y -qq install tesseract-ocr libtesseract-dev\n",
        "\n",
        "!sudo apt-get -y -qq install poppler-utils libxml2-dev libxslt1-dev antiword unrtf poppler-utils pstotext tesseract-ocr flac ffmpeg lame libmad0 libsox-fmt-mp3 sox libjpeg-dev swig\n",
        "\n",
        "!pip install langchain\n",
        "!pip install chromadb\n",
        "!pip install openai\n",
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VFIyqJz0CFai"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ['PATH'] += os.pathsep + '/usr/local/bin/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ooEP_qmgYjVJ"
      },
      "outputs": [],
      "source": [
        "!mkdir questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328,
          "referenced_widgets": [
            "4484b10f829c4dc88c1fbe121dd57f38",
            "d7f28755cd344b7891832202d483547b",
            "d921f229c2e2431685f4b40ad7b4aba9",
            "38694036aa9b4d839facd48484052c70"
          ]
        },
        "id": "Ku_9he0M9dkS",
        "outputId": "ddb93ab9-04bd-4c50-98b8-3e570004e422"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "WARNING:unstructured:This function will be deprecated in a future release and `unstructured` will simply use the DEFAULT_MODEL from `unstructured_inference.model.base` to set default model name\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4484b10f829c4dc88c1fbe121dd57f38",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "yolox_l0.05.onnx:   0%|          | 0.00/217M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d7f28755cd344b7891832202d483547b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/1.47k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d921f229c2e2431685f4b40ad7b4aba9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/115M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "38694036aa9b4d839facd48484052c70",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/46.8M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at microsoft/table-transformer-structure-recognition were not used when initializing TableTransformerForObjectDetection: ['model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
            "- This IS expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
        "# initializing the embeddings\n",
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "# default model = \"gpt-3.5-turbo\"\n",
        "llm = ChatOpenAI()\n",
        "\n",
        "directory = \"/content/questions\"\n",
        "\n",
        "def load_docs(directory):\n",
        "    loader = DirectoryLoader(directory)\n",
        "    try:\n",
        "        documents = loader.load()\n",
        "        return documents\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Directory not found: '{directory}'\")\n",
        "        return []\n",
        "\n",
        "documents = load_docs(directory)\n",
        "\n",
        "def split_docs(documents, chunk_size=500, chunk_overlap=50):\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "    )\n",
        "    docs = text_splitter.split_documents(documents)\n",
        "    return docs\n",
        "\n",
        "docs = split_docs(documents)\n",
        "\n",
        "db = Chroma.from_documents(\n",
        "    documents=docs,\n",
        "    embedding=embeddings\n",
        ")\n",
        "\n",
        "chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
        "\n",
        "def get_answer(query):\n",
        "    similar_docs = db.similarity_search(query, k=2) # get two closest chunks\n",
        "    answer = chain.run(input_documents=similar_docs, question=query)\n",
        "    return answer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4TkhQZkbZkH",
        "outputId": "b029244f-e022-4b2f-c2f2-5a9ddad21df1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Private Q&A chatbot\n",
            "Enter your query here: {     \"questions\": {         \"1\": \"Why are you interested in becoming a GDSC Lead?\",         \"10\": \"What innovative ideas do you have for growing and improving the GDSC community at your university?\",         \"2\": \"Can you share an example of a community project you have previously worked on?\",         \"3\": \"How would you motivate and encourage your peers to actively participate in GDSC activities?\",         \"4\": \"What technical skills do you possess that would be beneficial for leading a GDSC?\",         \"5\": \"How do you plan to collaborate with industry experts to enhance the learning experience for GDSC members?\",         \"6\": \"What strategies would you implement to ensure inclusivity and diversity within the GDSC community?\",         \"7\": \"How do you plan to measure the success and impact of GDSC activities in your university?\",         \"8\": \"Can you discuss a challenging situation you have faced in a leadership role and how you handled it?\",         \"9\": \"How would you balance your academic responsibilities with your role as a GDSC Lead?\"     } }. Give me the answers of these questions in json formt. The json format should be {<id>:{\"<question>\":\"<answer>\"}, <id>:{\"<question>\":\"<answer>\"}}\n",
            "Answer: {\n",
            "1:{\"Why are you interested in becoming a GDSC Lead?\":\"I am passionate about technology and community impact, and I believe leading a GDSC will allow me to combine these interests.\"},\n",
            "2:{\"Can you share an example of a community project you have previously worked on?\":\"I have worked on a coding project to help local businesses optimize their online presence.\"},\n",
            "3:{\"How would you motivate and encourage your peers to actively participate in GDSC activities?\":\"I would organize engaging events, highlight the benefits of participation, and recognize and celebrate the contributions of active members.\"},\n",
            "4:{\"What technical skills do you possess that would be beneficial for leading a GDSC?\":\"I have experience in computer programming and software engineering, which will be valuable for guiding GDSC members in technical projects.\"},\n",
            "5:{\"How do you plan to collaborate with industry experts to enhance the learning experience for GDSC members?\":\"I intend to invite industry experts to give talks, workshops, and mentorship sessions to provide real-world insights and guidance.\"},\n",
            "6:{\"What strategies would you implement to ensure inclusivity and diversity within the GDSC community?\":\"I will actively promote inclusivity, encourage diverse perspectives, and create a welcoming environment where all members feel valued and respected.\"},\n",
            "7:{\"How do you plan to measure the success and impact of GDSC activities in your university?\":\"I will track participation rates, project outcomes, member feedback, and community engagement to assess the effectiveness and impact of GDSC activities.\"},\n",
            "8:{\"Can you discuss a challenging situation you have faced in a leadership role and how you handled it?\":\"One challenging situation I faced was resolving conflicts within a team. I addressed it by facilitating open communication, understanding different viewpoints, and finding a mutually agreeable solution.\"},\n",
            "9:{\"How would you balance your academic responsibilities with your role as a GDSC Lead?\":\"I plan to prioritize and manage my time effectively, delegate tasks when necessary, and maintain clear communication with my team to ensure I fulfill both academic and GDSC responsibilities.\"}\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "print(\"Private Q&A chatbot\")\n",
        "prompt = input(\"Enter your query here: \")\n",
        "\n",
        "if prompt:\n",
        "    answer = get_answer(prompt)\n",
        "    print(f\"Answer: {answer}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#API\n",
        "Features\n",
        "- Ask questions\n",
        "- Evaluate answers\n",
        "- Analyse resume"
      ],
      "metadata": {
        "id": "IgZ6aehmlbY5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dr0Q4B8z616p",
        "outputId": "b0755310-e0a6-40d2-96b9-b773f1e5c4c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (4.2.0)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.11.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pypdf\n",
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQAFY08h_Wy-",
        "outputId": "9d3843d6-b484-4d3c-f641-a5aa59694edc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ngrok tunnel https://f95b-34-30-231-99.ngrok-free.app-> http://127.0.0.1:5000\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug:127.0.0.1 - - [14/Apr/2024 04:36:18] \"POST /generate_questions HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [14/Apr/2024 04:36:56] \"POST /evaluate_answers HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [14/Apr/2024 04:41:59] \"POST /evaluate_answers HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [14/Apr/2024 04:56:47] \"POST /generate_questions HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [14/Apr/2024 04:57:03] \"POST /evaluate_answers HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [14/Apr/2024 04:57:51] \"POST /generate_questions HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [14/Apr/2024 04:58:26] \"POST /evaluate_answers HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [14/Apr/2024 05:25:57] \"POST /generate_questions HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [14/Apr/2024 05:26:34] \"POST /evaluate_answers HTTP/1.1\" 200 -\n"
          ]
        }
      ],
      "source": [
        "from flask import Flask, request, jsonify\n",
        "import json\n",
        "from collections import OrderedDict\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from pyngrok import ngrok\n",
        "\n",
        "\n",
        "\n",
        "app = Flask(__name__)\n",
        "port = \"5000\"\n",
        "\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "public_url = !ngrok.connect(port)[0]\n",
        "public_url = ngrok.connect(port).public_url\n",
        "\n",
        "print(\"ngrok tunnel \" + public_url +  \"-> http://127.0.0.1:\" + port)\n",
        "\n",
        "# Update any base URLs to use the public ngrok URL\n",
        "app.config[\"BASE_URL\"] = public_url\n",
        "\n",
        "@app.route(\"/generate_questions\", methods=[\"POST\"])\n",
        "def generate_questions():\n",
        "    data = request.get_json()\n",
        "    job_role = data[\"job_role\"]\n",
        "    num_questions = data[\"num_questions\"]\n",
        "\n",
        "    # Generate questions based on job role and number of questions\n",
        "    questions_json = generate_questions_for_job_role(job_role, num_questions)\n",
        "\n",
        "    # Parse the JSON response\n",
        "    questions_dict = json.loads(questions_json)\n",
        "\n",
        "    # Sort the dictionary items by key as integers\n",
        "    sorted_questions = OrderedDict(sorted(questions_dict.items(), key=lambda x: int(x[0])))\n",
        "\n",
        "    # Return sorted questions in JSON format\n",
        "    return jsonify({\"questions\": sorted_questions})\n",
        "\n",
        "\n",
        "def generate_questions_for_job_role(job_role, num_questions):\n",
        "\n",
        "    questions = get_answer(f\"Generate {num_questions} question for a {job_role} interview. Return the output as JSON with the key as question numbers(1,2,3,4....)\")\n",
        "\n",
        "    return questions\n",
        "\n",
        "@app.route(\"/evaluate_answers\", methods=[\"POST\"])\n",
        "def evaluate_answers():\n",
        "    data = request.get_json()\n",
        "\n",
        "    # Convert the data to a string for inclusion in the f-string\n",
        "    data_str = json.dumps(data)\n",
        "\n",
        "    # Get the answer from the API\n",
        "    evaluations = get_answer(f\"\"\"Act as a reviewer.\n",
        "                             {data_str}\n",
        "                             Evaluate the answers given in this json and check the correctness of the answers and its positive and negaitve remarks.\n",
        "                             Return the response as a JSON with the format given below\n",
        "                             {{\n",
        "                              \"<id>\" : {{\n",
        "                                  \"<question>\":\"<Evaluation>\"\n",
        "                              }},\n",
        "                              .\n",
        "                              .\n",
        "                              .\n",
        "\n",
        "                            }}\"\"\")\n",
        "\n",
        "    try:\n",
        "        # Parse the JSON response\n",
        "        evaluation_dict = json.loads(evaluations)\n",
        "\n",
        "        # Sort the dictionary items by key as strings\n",
        "        sorted_evaluations = OrderedDict(sorted(evaluation_dict.items(), key=lambda x: x[0]))\n",
        "\n",
        "        # Return sorted evaluations in JSON format\n",
        "        return jsonify({\"evaluations\": sorted_evaluations})\n",
        "    except json.JSONDecodeError as e:\n",
        "        # Return an error response if the response from get_answer is not valid JSON\n",
        "        return jsonify({\"error\": f\"Error decoding JSON response: {e}\"})\n",
        "\n",
        "\n",
        "@app.route(\"/analyze_resume\", methods=[\"POST\"])\n",
        "def analyze_resume():\n",
        "    # Check if the request contains a file named 'resume'\n",
        "    if 'resume' not in request.files:\n",
        "        return jsonify({\"error\": \"No file found in the request\"}), 400\n",
        "\n",
        "    resume_file = request.files['resume']\n",
        "\n",
        "    # Ensure the file is a PDF\n",
        "    if resume_file.filename.endswith('.pdf'):\n",
        "        # Save the uploaded file to the /content/ directory\n",
        "        file_path = '/content/'+ resume_file.filename\n",
        "        resume_file.save(file_path)\n",
        "\n",
        "        # Load the PDF contents using PyPDFLoader\n",
        "        pdf_loader = PyPDFLoader(file_path)\n",
        "\n",
        "        # Extract the content from the first page (assuming it's a single-page resume)\n",
        "        resume_content = pdf_loader.load_and_split()[0].page_content\n",
        "\n",
        "        # Modify the custom prompt to include the resume content\n",
        "        prompt = f\"\"\"Analyze the resume and provide tips for improvement based on the following content:\n",
        "        {resume_content}\"\"\"\n",
        "\n",
        "        # Get the analysis using the modified prompt\n",
        "        analysis = get_answer(prompt)\n",
        "\n",
        "        # Remove the uploaded file after analysis\n",
        "        os.remove(file_path)\n",
        "\n",
        "        # Return the analysis as JSON\n",
        "        return jsonify({\"tips\": analysis})\n",
        "    else:\n",
        "        return jsonify({\"error\": \"Invalid file format. Only PDF files are supported.\"}), 400\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app.run()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3-lyexJYcEV"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}